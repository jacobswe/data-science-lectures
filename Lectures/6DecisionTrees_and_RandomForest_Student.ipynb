{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Trees\n",
    "\n",
    "Imagine you have a set of data you would like to classify. One potential solution to this problem would be to attempt to define rules that would allow you to do quickly evaluate where the best locations to split the data are. Then, imagine you could add rules to either side of the decision. This idea of branching based on features is the fundamental concept behind a decision tree.\n",
    "\n",
    "Let's visualize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import graphviz\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn import tree\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<seaborn.axisgrid.FacetGrid at 0x11fa70470>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 407.6x360 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Data Generation\n",
    "AB, response = datasets.make_blobs(n_samples=100, centers=2, n_features=2, center_box=(-2,2))\n",
    "\n",
    "data = {'A':[],'B':[],'R':[]}\n",
    "for i in AB:\n",
    "    data['A'].append(i[0])\n",
    "    data['B'].append(i[1])\n",
    "    data['R'].append(response[len(data['A'])-1])\n",
    "data = pd.DataFrame(data)\n",
    "\n",
    "sns.relplot(data=data, x='A',y='B', hue='R')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.40.1 (20161225.0304)\n",
       " -->\n",
       "<!-- Title: Tree Pages: 1 -->\n",
       "<svg width=\"238pt\" height=\"165pt\"\n",
       " viewBox=\"0.00 0.00 238.00 165.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 161)\">\n",
       "<title>Tree</title>\n",
       "<polygon fill=\"#ffffff\" stroke=\"transparent\" points=\"-4,4 -4,-161 234,-161 234,4 -4,4\"/>\n",
       "<!-- 0 -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>0</title>\n",
       "<polygon fill=\"none\" stroke=\"#000000\" points=\"168,-157 62,-157 62,-89 168,-89 168,-157\"/>\n",
       "<text text-anchor=\"middle\" x=\"115\" y=\"-141.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">X[1] &lt;= &#45;0.142</text>\n",
       "<text text-anchor=\"middle\" x=\"115\" y=\"-126.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">entropy = 1.0</text>\n",
       "<text text-anchor=\"middle\" x=\"115\" y=\"-111.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">samples = 100</text>\n",
       "<text text-anchor=\"middle\" x=\"115\" y=\"-96.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">value = [50, 50]</text>\n",
       "</g>\n",
       "<!-- 1 -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>1</title>\n",
       "<polygon fill=\"none\" stroke=\"#000000\" points=\"106,-53 0,-53 0,0 106,0 106,-53\"/>\n",
       "<text text-anchor=\"middle\" x=\"53\" y=\"-37.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">entropy = 0.953</text>\n",
       "<text text-anchor=\"middle\" x=\"53\" y=\"-22.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">samples = 59</text>\n",
       "<text text-anchor=\"middle\" x=\"53\" y=\"-7.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">value = [22, 37]</text>\n",
       "</g>\n",
       "<!-- 0&#45;&gt;1 -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>0&#45;&gt;1</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M93.1411,-88.9777C87.4932,-80.187 81.4007,-70.7044 75.7104,-61.8477\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"78.5356,-59.7698 70.1856,-53.2485 72.6463,-63.5536 78.5356,-59.7698\"/>\n",
       "<text text-anchor=\"middle\" x=\"64.8684\" y=\"-73.9765\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">True</text>\n",
       "</g>\n",
       "<!-- 2 -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>2</title>\n",
       "<polygon fill=\"none\" stroke=\"#000000\" points=\"230,-53 124,-53 124,0 230,0 230,-53\"/>\n",
       "<text text-anchor=\"middle\" x=\"177\" y=\"-37.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">entropy = 0.901</text>\n",
       "<text text-anchor=\"middle\" x=\"177\" y=\"-22.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">samples = 41</text>\n",
       "<text text-anchor=\"middle\" x=\"177\" y=\"-7.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">value = [28, 13]</text>\n",
       "</g>\n",
       "<!-- 0&#45;&gt;2 -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>0&#45;&gt;2</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M136.8589,-88.9777C142.5068,-80.187 148.5993,-70.7044 154.2896,-61.8477\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"157.3537,-63.5536 159.8144,-53.2485 151.4644,-59.7698 157.3537,-63.5536\"/>\n",
       "<text text-anchor=\"middle\" x=\"165.1316\" y=\"-73.9765\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">False</text>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.files.Source at 0x11fac10b8>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#One Deep\n",
    "t = DecisionTreeClassifier(max_depth=1,criterion='entropy')\n",
    "t.fit(data[['A','B']],data['R'])\n",
    "graphviz.Source(tree.export_graphviz(t, out_file=None)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entropy can be defined as a measure of homogeneity of a group where at $entropy=1$ you have an even mixture of observations and at $entropy=0$ you have a completely pure class. We won't dive to deep into the mathematics, but it can be mathematically expressed as $$entropy=\\sum_{i=1}^n(-p_ilog_2(p_i))$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#One Deep Decision Surface\n",
    "x_min, x_max = data['A'].min()-1, data['A'].max()+1\n",
    "y_min, y_max = data['B'].min()-1, data['B'].max()+1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, .01),np.arange(y_min, y_max, .01))\n",
    "Z = t.predict(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)\n",
    "plt.contourf(xx, yy, Z, cmap='RdBu')\n",
    "plt.scatter(x=data['A'],y=data['B'],c=data['R'],cmap='RdBu',vmin=-.2, vmax=1.2,edgecolors='white')\n",
    "plt.xlabel('A')\n",
    "plt.ylabel('B')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, to get more precision, we can increase the level of depth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Two Deep\n",
    "t = DecisionTreeClassifier(max_depth=2,criterion='entropy')\n",
    "t.fit(data[['A','B']],data['R'])\n",
    "graphviz.Source(tree.export_graphviz(t, out_file=None)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#One Deep Decision Surface\n",
    "x_min, x_max = data['A'].min()-1, data['A'].max()+1\n",
    "y_min, y_max = data['B'].min()-1, data['B'].max()+1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, .01),np.arange(y_min, y_max, .01))\n",
    "Z = t.predict(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)\n",
    "plt.contourf(xx, yy, Z, cmap='RdBu')\n",
    "plt.scatter(x=data['A'],y=data['B'],c=data['R'],cmap='RdBu',vmin=-.2, vmax=1.2,edgecolors='white')\n",
    "plt.xlabel('A')\n",
    "plt.ylabel('B')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Very Deep\n",
    "t = DecisionTreeClassifier(max_depth=15,criterion='entropy')\n",
    "t.fit(data[['A','B']],data['R'])\n",
    "x_min, x_max = data['A'].min()-1, data['A'].max()+1\n",
    "y_min, y_max = data['B'].min()-1, data['B'].max()+1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, .01),np.arange(y_min, y_max, .01))\n",
    "Z = t.predict(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)\n",
    "plt.contourf(xx, yy, Z, cmap='RdBu')\n",
    "plt.scatter(x=data['A'],y=data['B'],c=data['R'],cmap='RdBu',vmin=-.2, vmax=1.2,edgecolors='white')\n",
    "plt.xlabel('A')\n",
    "plt.ylabel('B')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Though we may increase the level of accuracy we can achieve (on our train data), what happens is that this is no longer fitting the general trends of the data, rather it is modeling the idiosyncrancries of our dataset. If we are to run a split and then test, we can find what the optimal level of depth is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(ncols=3,nrows=3,figsize=(15,15))\n",
    "\n",
    "for j in range(3):\n",
    "    #Data Generation\n",
    "    AB, response = datasets.make_moons(n_samples=500,noise=0.2*(j+1))\n",
    "\n",
    "    data = {'A':[],'B':[],'R':[]}\n",
    "    for i in AB:\n",
    "        data['A'].append(i[0])\n",
    "        data['B'].append(i[1])\n",
    "        data['R'].append(response[len(data['A'])-1])\n",
    "    data = pd.DataFrame(data)\n",
    "\n",
    "    xx, yy = np.meshgrid(np.arange(data['A'].min()-1, data['A'].max()+1, .01),\n",
    "                         np.arange(data['B'].min()-1, data['B'].max()+1, .01))\n",
    "\n",
    "    #Train/Test Split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(data[['A','B']], data['R'], test_size=0.2)\n",
    "\n",
    "    #Predictions\n",
    "    test_accuracy = []\n",
    "    train_accuracy = []\n",
    "    for i in range(30):\n",
    "        t = DecisionTreeClassifier(max_depth=i+1,criterion='entropy')\n",
    "        t.fit(X_train,y_train)\n",
    "        train = cross_validate(t,X=X_test,y=y_test,scoring='roc_auc',\n",
    "                               cv=3,return_train_score=True)\n",
    "        test_accuracy.append(sum(train['test_score'])/len(train['test_score']))\n",
    "        train_accuracy.append(sum(train['train_score'])/len(train['train_score']))\n",
    "    accs = pd.DataFrame({'Test':test_accuracy, 'Train':train_accuracy}).set_index(np.linspace(1,31,30))\n",
    "\n",
    "    #Create Plots  \n",
    "    sns.scatterplot(data=data, x='A',y='B', hue='R',ax=axs[j,0]).set(title='Data Set')\n",
    "    sns.lineplot(data=accs,ax=axs[j,1]).set(title='Accuracy over Depth')\n",
    "\n",
    "    #Plot Contour\n",
    "    bestDepth = test_accuracy.index(max(test_accuracy))\n",
    "    t = DecisionTreeClassifier(max_depth=bestDepth+1,criterion='entropy')\n",
    "    t.fit(X_train,y_train)\n",
    "    Z = t.predict(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)\n",
    "\n",
    "    axs[j,2].contourf(xx, yy, Z, cmap='RdBu')\n",
    "    axs[j,2].scatter(x=data['A'],y=data['B'],c=data['R'],cmap='RdBu',vmin=-.2, vmax=1.2,edgecolors='white')\n",
    "    axs[j,2].set(xlabel='A',ylabel='B',title='Depth'+str(bestDepth+1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You're not required to shoot in the dark as it come to customization of your tree. There exists something called a hyperparameter. Recall from our session on logistic regression that there are certain inputs you give your model that are not driven by the data - rather selected explicitly. In the case of the hyperparameter $C=\\frac{1}{\\lambda}$ we tune the level of punishment we give for complexity. This type of change results in a different model for each value we give. In the case of a decision tree, we have the following hyperparameters:\n",
    "\n",
    "1) **max_depth**: This is what we see as a the maximum number of layers to go down in the tree, i.e. max splits\n",
    "\n",
    "2) **min_samples_split**: The minimum number or ratio of samples required to have a split, default = 2\n",
    "\n",
    "3) **min_samples_leaf**: The minimum number of samples in a leaf (i.e. end point), default = 1\n",
    "\n",
    "4) **max_features**: The maximum number of features to include, default = unlimited\n",
    "\n",
    "5) **max_leaf_nodes**: The maximum number of leafs allowed, default = unlimited\n",
    "\n",
    "6) **criterion**: 'gini' or 'entropy' (Impurity or Information Gain)\n",
    "\n",
    "There are no hard rules as to what works best - much of this is heavily dependent on the size and quality of your data. For example with millions of data points, you likely don't have a need for leafs with only one observation. Likewise, for small datasets, you will likely find some gain from having that be allowed. This part of model creation is more art than science and requires experimentation.\n",
    "\n",
    "## Regression\n",
    "You can also use this sort of method for regression - to predict a number rather than a category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate Data\n",
    "X = np.random.randn(200)*2\n",
    "Y = list(map(lambda x: np.sin(x) + np.random.randn(1)[0]/10,X))\n",
    "sns.relplot(x='X',y='Y',data=pd.DataFrame({'X':X,'Y':Y}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate Decision Trees\n",
    "fig, axs = plt.subplots(ncols=3,nrows=2,figsize=(15,12))\n",
    "\n",
    "c = 0\n",
    "r = 0\n",
    "for i in range(6):\n",
    "    t = DecisionTreeRegressor(max_depth=i+1,min_samples_leaf=4)\n",
    "    t.fit(np.array(X).reshape(-1,1),Y)\n",
    "    score = t.score(np.array(X).reshape(-1,1),Y)\n",
    "    sns.scatterplot(x='X',y='Y',data=pd.DataFrame({'X':X,'Y':Y}),\n",
    "                    ax=axs[r,c]).set(title='Depth: '+str(i+1)+', Score: '+str(round(score,2)))\n",
    "    p = t.predict(np.array(X).reshape(-1,1))\n",
    "    sns.lineplot(x='X',y='P',data=pd.DataFrame({'X':X,'P':p}),ax=axs[r,c],color='r')\n",
    "    if c == 2 :\n",
    "        c=0\n",
    "        r+=1\n",
    "    else:\n",
    "        c+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuning the Hyperparameters\n",
    "\n",
    "How we saw above where there is such a large number of tunable variables, we would like to develop a way to analyze the results of our changes to these, and eventually develop a programmatic method for evaluating our success. We will find that the process can be automated to an extent, but as it comes to the complexity accuracy tradeoff, it will become a game of experience and feeling.\n",
    "\n",
    "Let's start by remembering our methods of tuning that we developed during the Logistic Regression session, but for a decision tree. For now, we consider only max_depth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate Data\n",
    "ABCD, response = datasets.make_blobs(n_samples=500, centers=2,\n",
    "                                     n_features=4, center_box=(-2,2))\n",
    "X_train, X_test, y_train, y_test = train_test_split(ABCD, response, test_size=0.2)\n",
    "\n",
    "depths = [1,2,3,4,5,10,15,20,25,30]\n",
    "accs = []\n",
    "for i in depths:\n",
    "    dtc = DecisionTreeClassifier(max_depth=i)\n",
    "    dtc.fit(X_train,y_train)\n",
    "    accs.append(dtc.score(X_test,y_test))\n",
    "    \n",
    "print('Scores:\\n',pd.DataFrame({'Accuracy':accs},index=depths))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this, we can quickly see how our data is behaving as we increase the depth of the trees. One thing we might also be interested in doing is evaluating this as a cross-validation to make sure that we are getting what is as close to a real number as possible. This may look something like the following."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate Data\n",
    "ABCD, response = datasets.make_blobs(n_samples=500, centers=2,\n",
    "                                     n_features=4, center_box=(-2,2))\n",
    "X_train, X_test, y_train, y_test = train_test_split(ABCD, response, test_size=0.2)\n",
    "\n",
    "depths = [1,2,3,4,5,10,15,20,25,30]\n",
    "accs = []\n",
    "for i in depths:\n",
    "    dtc = DecisionTreeClassifier(max_depth=i)\n",
    "    accs.append(cross_val_score(dtc,ABCD,response,cv=5).mean())\n",
    "    \n",
    "print('Scores:\\n',pd.DataFrame({'Accuracy':accs},index=depths))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the situation that we would like to evaluate more than just the depth of the tree, for example, also the min_samples_leaf, we can nest some for loops to create this behaviors. Let's see."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate Data\n",
    "ABCD, response = datasets.make_blobs(n_samples=500, centers=2,\n",
    "                                     n_features=4, center_box=(-2,2))\n",
    "X_train, X_test, y_train, y_test = train_test_split(ABCD, response, test_size=0.2)\n",
    "\n",
    "depths = [1,2,3,4,5,10,15,20,25,30]\n",
    "leafs = [10,20,30,40,50,60]\n",
    "accs = pd.DataFrame({'Depth':[],'Leaf Size':[],'Accuracy':[]})\n",
    "for i in depths:\n",
    "    for j in leafs:\n",
    "        dtc = DecisionTreeClassifier(max_depth=i,min_samples_leaf=j)\n",
    "        accs = accs.append({'Depth':i,'Leaf Size':j,\n",
    "                     'Accuracy':cross_val_score(dtc,ABCD,response,cv=5).mean()},ignore_index=True)\n",
    "    \n",
    "print('Scores:\\n',accs.sort_values('Accuracy',ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can continue to do this sort of methodology with $n$ nested for-loops for $n$ hyperparameters - but who has time for that. SKLearn has an in built feature to do exactly this sort of analysis. This method, called GridSearchCV does more or less exactly what we have been exploring, but with even more features. The way you pass the sets of values you would like to evaluate over is very similar to what we have done so far. Just like in doing our decision trees, we can see that the relationship between the number of hyperparameters and the computation time is exponential.\n",
    "\n",
    "The time to perform these computations has the following form where $\\mid P_k\\mid$ is the length of the list of potential values for each parameter:\n",
    "\n",
    "$$Computation\\ Time=\\left(\\prod_{k=1}^{n}\\mid P_k\\mid\\right)\\times CV,\\ n\\ parameters$$\n",
    "\n",
    "Let's do the same model as above, but with this new technique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ABCD, response = datasets.make_blobs(n_samples=500, centers=2,\n",
    "                                     n_features=4, center_box=(-2,2))\n",
    "\n",
    "dtc = DecisionTreeClassifier(criterion='entropy') #Note, you can declare a constant hyperparameter in your model\n",
    "grid = {'max_depth':[1,2,3,4,5,10,15,20,25,30],\n",
    "        'min_samples_leaf':[10,20,30,40,50,60]}\n",
    "\n",
    "gs = GridSearchCV(dtc,grid,cv=5,verbose=True,return_train_score=False)\n",
    "gs.fit(ABCD,response)\n",
    "\n",
    "scores = pd.DataFrame(gs.cv_results_).filter(regex='param_+|mean_test_score'\n",
    "                                            ).sort_values('mean_test_score',\n",
    "                                                          ascending=False).reset_index().drop(['index'],axis=1)\n",
    "scores.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's important to note that this final method to declare scores uses a powerful tool called a regular expression where you can match text based on certain conditions. In our case, we are saying to match anything that starts with 'param_' and has at least one character after that, or something that is exactly 'mean_test_score'. What this gives us is each of our hyperparameters (which are now param_name).\n",
    "\n",
    "Now, as an exercise, you will modify the code that we used to analyze two hyperparameters, and now also include the 'criterion' hyperparameter. Please consult the documentation for details on its possible values. Then print out exactly the same type of table as above with this new column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ABCD, response = datasets.make_blobs(n_samples=500, centers=2,\n",
    "                                     n_features=4, center_box=(-2,2))\n",
    "\n",
    "dtc = DecisionTreeClassifier()\n",
    "grid = {'max_depth':[1,2,3,4,5,10,15,20,25,30],\n",
    "        'min_samples_leaf':[10,20,30,40,50,60],\n",
    "        'new_param':['list','of','values']}\n",
    "\n",
    "gs = GridSearchCV(dtc,grid,cv=5,verbose=True,return_train_score=False)\n",
    "gs.fit(ABCD,response)\n",
    "\n",
    "scores = pd.DataFrame(gs.cv_results_).filter(regex='param_+|mean_test_score'\n",
    "                                            ).sort_values('mean_test_score',\n",
    "                                                          ascending=False).reset_index().drop(['index'],axis=1)\n",
    "scores.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It might be tempting to simply say, we want to maximize our accuracy so we choose the best performing model. Now, in some cases this may be what you end up doing, but more often, you will select a model that strikes a balance between complexity and accuracy.\n",
    "\n",
    "Let's discuss a theoretical example. If you were to have a model that is evaluating max_depth and min_samples_leaf, what you would like to do is predict accurately and simply to avoid overfitting. One method for this is to do a cost-benefit analysis on the $\\Delta complexity\\ $to$\\ \\Delta accuracy$ relationship.\n",
    "\n",
    "If our output from the grid search were to give us something like: (max_depth,min_samples_leaf,accuracy)=(10,50,.8),(15,50,.802) we must make a decision of which one is better. In thinking of this in terms of the above relationship, we have a percentage change of 50% in max_depth, with a resulting percentage change of 0.25% in accuracy. It should be a relatively safe bet that this change in accuracy would likely fall within a CI of accuracy based on our cross validation, or at least is so insignificant (not in the statistical way, but philosophical way) that including 50% more complexity is not worth it.\n",
    "\n",
    "Based on this, let's go ahead and try to select what we think the best model might be out of scores. Note, there is no right answer to this question (though I would argue there are wrong answers)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we can understand decision trees and build a functional, well-tuned model, let's move on to something that is a little bit more complex."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest\n",
    "Just like how having one decision tree is a good way to model data, if you have many simultaneous trees, you can find even better predictive power, especially in situations where you have many features. Similarly to all of the models that we have seen, this can overfit as well. Let's go over the techniques for preparing, analyzing, and predicting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data Generation\n",
    "AB, response = datasets.make_blobs(n_samples=1000, centers=2, n_features=5, center_box=(-2,2))\n",
    "X_train, X_test, y_train, y_test = train_test_split(AB, response, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, we are dealing with 2 classes, but the process can be generalized to n classes. Let's start with a single tree. This is declared using the n_estimators parameter meaning the number of trees we are using to do our estimation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc = RandomForestClassifier(max_depth=10,n_estimators=5)\n",
    "rfc.fit(X_train,y_train)\n",
    "print(rfc.score(X_test,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, this behaves exactly like any other model in SKLearn. In fact, there are some methods shared by nearly all models: fit and score. Using these, you can quickly change and evaluate different types of models, something that may come in useful over the following days while you fit your own models.\n",
    "\n",
    "It should be noted that as we increase the complexity of our machine learning models, what we are going to find is that computation will take longer and longer. Oftentimes, in preparing my own models I will be tuning hyperparameters on a time scale measured in hours rather than in miliseconds as we have had so far. If your projects end up requiring a significant amount of processing power, we can consult on techniques to offload this CPU demand to the cloud - but for now we will simply suffer the waiting.\n",
    "\n",
    "Below you will see some skeleton code for how to implement the grid search over our newfound parameter for the number of trees. It will be your task to flesh out this code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xs, y = datasets.make_classification(n_samples=500,n_features=5,n_redundant=1,n_informative=4)\n",
    "\n",
    "rfc = RandomForestClassifier() \n",
    "grid = {'param':['list'],\n",
    "        'param':['list'],\n",
    "        'param':['list']}\n",
    "\n",
    "gs = GridSearchCV(rfc,grid,cv=5,verbose=True,return_train_score=False)\n",
    "gs.fit(Xs,y)\n",
    "\n",
    "scores = pd.DataFrame(gs.cv_results_).filter(regex='param_+|mean_test_score'\n",
    "                                            ).sort_values('mean_test_score',\n",
    "                                                          ascending=False).reset_index().drop(['index'],axis=1)\n",
    "scores.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just like how we saw the numerical prediction equivalent to DecisionTreeClassifier is DecisionTreeRegressor, the numerical predictor for RandomForestClassifier is RandomForestRegressor. There is really no difference between how you evaluate the accuracy of a classifier versus a regression model for RandomForest, and you gain a little more freedom than in pure regressions as the assumptions are relaxed. In general, so long as the model performs well for regression, all is okay.\n",
    "\n",
    "Let's see a quick example of how we can do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xs, y = datasets.make_regression(n_samples=500,n_features=5,n_informative=4,noise=0.5)\n",
    "\n",
    "rfr = RandomForestRegressor() \n",
    "grid = {'n_estimators':[1,2,3,4,5,10,15,20],\n",
    "        'max_depth':[1,2,3,4,5,6,7,8,9,10],\n",
    "        'min_samples_leaf':[10,20,30,40,50]}\n",
    "\n",
    "gs = GridSearchCV(rfr,grid,cv=5,verbose=True,return_train_score=False)\n",
    "gs.fit(Xs,y)\n",
    "\n",
    "scores = pd.DataFrame(gs.cv_results_).filter(regex='param_+|mean_test_score'\n",
    "                                            ).sort_values('mean_test_score',\n",
    "                                                          ascending=False).reset_index().drop(['index'],axis=1)\n",
    "scores.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As individuals, please now select which model you think performs the best based on what you can see in the scores and relative complexity - note that the number of estimators is a huge complexity factor.\n",
    "\n",
    "Please then make a plot of the residuals for this graph by plotting the predicted values against the real values of y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have built out well tuned models for both regression and classification, you are ready to go out and build for real!\n",
    "\n",
    "Before that though, let's speak briefly on how we should go about evaluating our successes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating Accuracy\n",
    "The below will illustrate three very similar measures of accuracy that you will select based on what sort of performance you care about in your model. Generally, the behavior of one will be highly correlated to the behavior of another with them often having exactly the same model all the way through to the thousands place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train/Test Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(AB, response, test_size=0.2)\n",
    "\n",
    "#Predictions\n",
    "accuracy = []\n",
    "precision = []\n",
    "roc = []\n",
    "for i in range(30):\n",
    "    t = DecisionTreeClassifier(max_depth=i+1,criterion='entropy')\n",
    "    t.fit(X_train,y_train)\n",
    "    accuracy.append(accuracy_score(y_test,t.predict(X_test)))\n",
    "    precision.append(precision_score(y_test,t.predict(X_test),average='macro'))\n",
    "    roc.append(roc_auc_score(y_test,t.predict(X_test),average='macro'))\n",
    "\n",
    "#Create Plots  \n",
    "fig, axs = plt.subplots(ncols=3,figsize=(15,5))\n",
    "sns.lineplot(x='Depth',y='Accuracy',data=pd.DataFrame({'Depth':np.linspace(1,31,30),'Accuracy':accuracy}),\n",
    "             ax=axs[0]).set(title='Prediction Accuracy over Depth')\n",
    "sns.lineplot(x='Depth',y='Precision',data=pd.DataFrame({'Depth':np.linspace(1,31,30),'Precision':precision}),\n",
    "             ax=axs[1]).set(title='Prediction Precision over Depth')\n",
    "sns.lineplot(x='Depth',y='AUC',data=pd.DataFrame({'Depth':np.linspace(1,31,30),'AUC':roc}),\n",
    "             ax=axs[2]).set(title='Prediction AUC over Depth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we see are three different calculations for how to evaluate the success of a model.\n",
    "\n",
    "$$tp=True\\ Positive,\\ tn=True\\ Negative,\\ fp=False\\ Positive,\\ fn=False\\ Negative$$\n",
    "\n",
    "$$Accuracy = \\frac{tp+tn}{tp+tn+fp+fn}$$\n",
    "\n",
    "$$Precision = \\frac{tp}{tp+fp}$$\n",
    "\n",
    "$$AUC=Area\\ Under\\ ROC\\ Curve$$\n",
    "\n",
    "Let us quickly also review what the ROC AUC means."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xs, y = datasets.make_moons(n_samples=500,noise=0.3)\n",
    "\n",
    "#Train/Test Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(Xs, y, test_size=0.2)\n",
    "\n",
    "#Predictions\n",
    "rfc = RandomForestClassifier(max_depth=10,n_estimators=5).fit(X_train,y_train)\n",
    "aucscore = metrics.roc_auc_score(rfc.predict(X_test),y_test)\n",
    "\n",
    "##Computing false and true positive rates\n",
    "fpr, tpr,_= metrics.roc_curve(rfc.predict(X_test),y_test,drop_intermediate=False)\n",
    "\n",
    "##Adding the ROC\n",
    "plt.plot(fpr, tpr, color='red',lw=2, label='ROC curve')\n",
    "plt.plot([0, 1], [0, 1], color='blue', lw=2, linestyle='--')\n",
    "plt.xlabel('FPR')\n",
    "plt.ylabel('TPR')\n",
    "plt.title('ROC Curve AUC = '+str(aucscore.round(4)))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing Random Forest Model Selection\n",
    "The below heatmaps show each of the measures of accuracy based on a set of heatmaps that have color representing the predictive capability of the model. This is an easy way of being able to quickly spot which families of models are likely going to be the best performing. As you move away from the origin, you would like to maximize the green while minimizing the distance you have to travel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train/Test Split\n",
    "AB, response = datasets.make_moons(n_samples=500,noise=0.5)\n",
    "X_train, X_test, y_train, y_test = train_test_split(AB, response, test_size=0.40)\n",
    "\n",
    "scores = {'Accuracy':[],'Precision':[],'ROC':[]}\n",
    "for i in range(30):\n",
    "    scores_acc = []\n",
    "    scores_pre = []\n",
    "    scores_roc = []\n",
    "    for j in range(20):\n",
    "        rf = RandomForestClassifier(n_estimators=i+1,max_depth=j+1)\n",
    "        rf.fit(X_train,y_train)\n",
    "        splits=3\n",
    "        scores_acc.append(accuracy_score(y_test,rf.predict(X_test)))\n",
    "        scores_pre.append(precision_score(y_test,rf.predict(X_test),average='micro'))\n",
    "        scores_roc.append(roc_auc_score(y_test,rf.predict(X_test),average='micro'))\n",
    "    scores['Accuracy'].append(scores_acc)\n",
    "    scores['Precision'].append(scores_pre)\n",
    "    scores['ROC'].append(scores_roc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(ncols=3,figsize=(16,10))\n",
    "cbar_ax = fig.add_axes([.91,.3,.03,.4])\n",
    "\n",
    "for i in range(3):\n",
    "    sns.heatmap(scores[list(scores.keys())[i]],xticklabels=np.arange(20)+1,yticklabels=np.arange(30)+1,\n",
    "                cmap='RdYlGn',ax=axs[i],cbar = i==0,cbar_ax=None if i else cbar_ax)\n",
    "    axs[i].set(title=list(scores.keys())[i],xlabel='Tree Depth',ylabel='Number of Trees')\n",
    "    axs[i].set_xticklabels(np.arange(20)+1,rotation=0)\n",
    "    axs[i].invert_yaxis()\n",
    "\n",
    "fig.tight_layout(rect=[0, 0, .9, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can now select and predict based on whichever tree you feel may give you the best outcome. In our case, accuracy is likely ging to be the deciding factor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Final Test\n",
    "\n",
    "We would like to now be able to check our understanding. Please take 15-30 minutes to build out each of these regression and classification models using the automated tuning system and select what you think the best model is and why. Then please discuss in your group why you chose that model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xs, y = datasets.make_classification(n_samples=500,n_features=5,n_redundant=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xs, y = datasets.make_regression(n_samples=500,n_features=5,noise=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
