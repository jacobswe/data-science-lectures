{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "from sklearn import datasets, metrics\n",
    "from sklearn.metrics import roc_curve, auc, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import label_binarize\n",
    "\n",
    "from scipy import interp\n",
    "from itertools import cycle\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this review is to bring to the forefront of your mind the workings of a Sequential model in Keras. This is the same methodology we have used for all of the non SKLearn Neural Networks that have been discussed. Simply put, the type is eponymous - we sequentially evaluate data over layers, in the order specified, first training to minimize our loss function using gradient descent by wiggling our weights and biases, then evaluating (testing) to discover our expected performance on new samples. By this point, you should have a strong understanding of at least these concepts and hopefully some confidence in being able to conquer the problems of loss selection, link function choices, data segmentation for information leakage control, metric and business problem identification, or others!\n",
    "\n",
    "Please break into your groups and work through the following two problems using the information in previous notebooks or your own experimentation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) Classification\n",
    "\n",
    "The following data has been created to perform a multiclass classification process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = datasets.make_classification(n_samples=5000, n_features=5, n_informative=5, n_redundant=0,\n",
    "                                    n_clusters_per_class=1, n_classes=5)\n",
    "X_train, X_test, y_train_l, y_test_l = train_test_split(X, y, test_size=0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorFlow expects our categorical information in the form of a one-hot-encoded array. They however are nice enough to provide a method for precisely this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before:  1\n",
      "After:  [0. 1. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "y_train = to_categorical(y_train_l)\n",
    "y_test = to_categorical(y_test_l)\n",
    "\n",
    "print('Before: ',y_train_l[0])\n",
    "print('After: ',y_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Declare your Sequential model and add three ReLU (10) layers. Then, determine and create the output layer.\n",
    "Disregard any 'colocate_with' errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.metrics.categorical_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Determine the correct loss function. Then, compile your model with an Adam optimizer and the categorical accuracy metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now, we are ready to fit. Please fit the model to the training data for 30 epochs.\n",
    "Remember to save the history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the loss of the training and testing data over epochs.\n",
    "Be careful about your indexing!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the accuracy metric of the training and testing data over epochs.\n",
    "Be careful about your indexing!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I won't make you create the ROC curves and Confusion Matrices as this has been reviewed before. In cases of actual analysis of model perfomance though, this step of TPR/FPR analysis is not optional. Should you have extra time after completing the rest of the workbook, please return to this step and build about the above using previously shared code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) Regression\n",
    "The following data has been created for regression, note that one-hot-encoding is no longer required or even conceivable on continuous (or at least ordinal) response data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = datasets.make_regression(n_samples=5000, n_features=10, noise=2)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Declare your Sequential model (identifying the valid input shape) and add three ReLU (15) layers. Then, determine and create the output layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select what you determine to be the correct loss function. Then, compile your model with an Adam optimizer and the MSE, MAE, and MAPE metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now, we are ready to fit. Please fit the model to the training data for 30 epochs.\n",
    "Remember to save the history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the loss of the training and testing data over epochs.\n",
    "Be careful about your indexing!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the three accuracy metrics of the training and testing data over epochs in side-by side plots.\n",
    "Be careful about your indexing!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
